import os
import openai
import json
import re

from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)  # ë” ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í–¥ìƒ

Responder_PROMPT = """
You are a highly capable data analyst working with tabular data.

Your task is to read the following table and answer the question as accurately as possible, using only the information in the table.

### Instructions
1. **Read the table carefully** - Each row is separated by "|" and each field within a row is separated by ";"
2. **Identify all conditions** in the question that need to be satisfied simultaneously
3. **For each row**, check if **ALL conditions are strictly matched (==)** using exact string comparison
4. **Extract the answer** - This should be one of the column values, such as a name, number, or label.
5. **Only use the table information provided**, do not guess or infer from external knowledge.
6. **Format your response** as a **JSON list**
7. **If no rows satisfy all conditions**, return: ["Answer not found in the table."]

---

### Step-by-step reasoning:

1. **Identify what is being asked for** â†’ what column name should be returned? (e.g., Band, Area_served, Purpose)
2. **List all conditions that must be matched** â†’ extract conditions as exact comparisons, e.g., Callsign == 4BCB
3. **Go through each row**:
    - For each row, check if **ALL conditions match exactly**
    - If yes, collect the value from the **target column**
4. **Prepare final answer** â†’ collect the answers into a JSON list format

---

### Table
{table_text}

### Question
{question}

---

### Analysis
Let me work through this step by step:

1. **Target column to extract** (must be one of the table column names):
2. **Conditions to match** (exact string comparison only):
3. **Row-by-row check**:
4. **Extracted answers**:

### Final Answer (as JSON list only)
"""

def extract_json_list(text: str) -> list:
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    # JSON ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
    json_pattern = r'\[.*?\]'
    matches = re.findall(json_pattern, text, re.DOTALL)
    
    for match in matches:
        try:
            # JSON íŒŒì‹± ì‹œë„
            parsed = json.loads(match)
            if isinstance(parsed, list):
                return parsed
        except json.JSONDecodeError:
            continue
    
    # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë‹¤ë¥¸ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
    answer_pattern = r'(?:Answer|answer):\s*(\[.*?\])'
    match = re.search(answer_pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # ë§ˆì§€ë§‰ ì¤„ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
    lines = text.strip().split('\n')
    for line in reversed(lines):
        if '[' in line and ']' in line:
            try:
                # ì¤„ì—ì„œ JSON ë¦¬ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                start = line.find('[')
                end = line.rfind(']') + 1
                potential_json = line[start:end]
                return json.loads(potential_json)
            except (json.JSONDecodeError, ValueError):
                continue
    
    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
    return ["Answer not found in the table."]

'''
Final LLM Responder Robustness ì—…ê·¸ë ˆì´ë“œ ì „ëµ
1. LLMì—ê²Œ ì´ì „ì˜ ì „ëµì²˜ëŸ¼ questionì˜ ì¡°ê±´ë¶€ì˜ ANDë¥¼ ì˜ ì¡ì•„ë‚´ê³ , ì£¼ì–´ì§„ rowsë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¹„êµí•˜ë©´ì„œ ì¶”ë¡ í•˜ë„ë¡ ìœ ë„í•˜ëŠ” prompting í•„ìš”
2. ë³µìˆ˜ ì‘ë‹µì¸ë° ë‹¨ìˆ˜ ì‘ë‹µì„ í•˜ê±°ë‚˜ ë°˜ëŒ€ë¡œ ë‹¨ìˆ˜ ì‘ë‹µì¸ë° ë³µìˆ˜ ì‘ë‹µì„ í•˜ëŠ” ë“±ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ì™„í•´ì•¼í•¨
'''
def responder_fn(state: dict) -> dict:
    question = state.get("question", "")
    answer = state["answer"]
    final_table_text = state.get("final_table_text", "")

    print(f"[LLM Responder] â“ Question:\n{question}")
    print(f"[LLM Responder] ğŸ“‹ Table:\n{final_table_text}")

    # Column descriptions ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    selected_column_descriptions = state.get("column_description", {})
    column_desc_text = ""
    if selected_column_descriptions:
        column_desc_text = "\n\n### Column Descriptions\n" + "\n".join(
            f"- {col}: {desc}" for col, desc in selected_column_descriptions.items()
        )

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    full_prompt = Responder_PROMPT + column_desc_text
    
    try:
        response = llm.invoke(
            full_prompt.format(
                question=question,
                table_text=final_table_text
            )
        )
        
        raw_answer = response.content.strip()
        print(f"[LLM Responder] ğŸ¤– Raw Response:\n{raw_answer}")
        
        # JSON ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        parsed_answer = extract_json_list(raw_answer)

        # Try to extract predicted entity type if specified in response
        predicted_entity_type = None
        entity_type_match = re.search(r'"?predicted_answer_entity"?\s*[:=]\s*"?(.*?)"?[\n,}]', raw_answer, re.IGNORECASE)
        if entity_type_match:
            predicted_entity_type = entity_type_match.group(1).strip()

        print(f"[LLM Responder] ğŸ§  Predicted Entity Type: {predicted_entity_type}")
        
        print(f"[LLM Responder] âœ… Parsed Answer: {parsed_answer}")
        print(f"\n [LLM Responder] âœ… Real Answer: {answer}")
        
        return {**state, "LLM_answer": parsed_answer, "predicted_answer_entity": predicted_entity_type}
        
    except Exception as e:
        print(f"[LLM Responder] âŒ Error: {e}")
        return {**state, "LLM_answer": ["Answer not found in the table."], "predicted_answer_entity": None}

# ë…¸ë“œ ìƒì„±
responder_node = RunnableLambda(responder_fn)