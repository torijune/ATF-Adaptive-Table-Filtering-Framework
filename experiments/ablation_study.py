import json
import pandas as pd
from langchain_core.runnables import RunnableLambda
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from rank_bm25 import BM25Okapi
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
import os

def get_data(index):
    jsonl_path = "outputs/tcrf_results.jsonl"
    target_index = index
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            entry = json.loads(line)
            # indexê°€ ë¬¸ìì—´ì´ë‚˜ ì •ìˆ˜ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¹„êµ ì‹œ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜
            if float(entry.get("index")) == float(target_index):
                answer = entry['answer']
                question = entry['question']
                raw_table = pd.DataFrame(entry['raw_table'])
                filtered_columns = entry['filtered_columns']
                column_relevance_scores = entry['column_relevance_scores']
                top_row_indices = entry['top_row_indices']

    return raw_table, filtered_columns, answer, question, column_relevance_scores, top_row_indices

def normalize_softmax(arr):
    arr = np.array(arr)
    e_x = np.exp(arr - np.max(arr))
    return e_x / e_x.sum()

# w/o column filtering
def row_column_filtering(index):

    raw_table, filtered_columns, answer, question, column_relevance_scores, top_row_indices = get_data(index)

    column_filtered_df = raw_table[filtered_columns]
    
    # ì „ì²´ rowë¥¼ questionê³¼ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë­í‚¹
    row_texts = raw_table.astype(str).agg(" ".join, axis=1).tolist()

    # TF-IDF + cosine similarity
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform([question] + row_texts)
    question_vector = vectors[0]
    row_vectors = vectors[1:]
    tfidf_sim = cosine_similarity(question_vector, row_vectors)[0]

    # BM25
    tokenized_rows = [text.lower().split() for text in row_texts]
    tokenized_query = question.lower().split()
    bm25 = BM25Okapi(tokenized_rows)
    bm25_scores = bm25.get_scores(tokenized_query)

    # Dense
    dense_model = SentenceTransformer("all-MiniLM-L6-v2")
    dense_question_emb = dense_model.encode(question, convert_to_tensor=True)
    dense_row_embs = dense_model.encode(row_texts, convert_to_tensor=True)
    dense_sim = util.pytorch_cos_sim(dense_question_emb, dense_row_embs)[0].cpu().numpy()

    # Score fusion
    tfidf_norm = normalize_softmax(tfidf_sim)
    bm25_norm = normalize_softmax(bm25_scores)
    dense_norm = normalize_softmax(dense_sim)
    fusion_scores = 0.4 * tfidf_norm + 0.3 * bm25_norm + 0.3 * dense_norm

    # Top-K row ì„ íƒ
    top_k_ratio = 0.4
    top_k = max(1, int(np.ceil(len(row_texts) * top_k_ratio)))
    top_indices = fusion_scores.argsort()[-top_k:][::-1]
    row_filtered_df = raw_table.iloc[top_indices].reset_index(drop=True)

    print("\nRaw Table Shape: ", raw_table.shape)
    print("\nw/o Row Filtering Table Shape: ", column_filtered_df.shape)
    print("\nw/o Column Filtering Table Shape: ", row_filtered_df.shape)
    print("Raw table column length: ", len(raw_table.columns), "Raw table row length: ", len(raw_table))
    print("w/o Row filter row length: ", len(column_filtered_df), "w/o Column filter column length: ", len(row_filtered_df.columns))


    return column_filtered_df, row_filtered_df, answer, question

def Top_k_vs_KMenas(index):

    raw_table, filtered_columns, answer, question, column_relevance_scores, top_row_indices = get_data(index)

    # âœ… L2 Norm ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    combined_scores = {
        col: np.linalg.norm(score)
        for col, score in column_relevance_scores.items()
    }

    # âœ… ì ìˆ˜ ëª©ë¡ ì¶”ì¶œ
    all_scores = list(combined_scores.values())

    # âœ… ì „ì²´ ì»¬ëŸ¼ ìˆ˜ ê³„ì‚°
    num_columns = len(combined_scores)

    # âœ… ì¡°ê±´ì— ë”°ë¥¸ ì„ê³„ê°’ ë˜ëŠ” Top-3 ì„ íƒ
    if num_columns >= 10:
        threshold = np.percentile(all_scores, 60)  # ìƒìœ„ 40% â†’ í•˜ìœ„ 60%ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ
        selected_columns = [col for col, score in combined_scores.items() if score >= threshold]
    else:
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 3ê°œ ì„ íƒ
        selected_columns = sorted(combined_scores, key=combined_scores.get, reverse=True)[:3]

    print(f"[DEBUG] index: {index}")
    print(f"[DEBUG] raw_table.columns: {list(raw_table.columns)}")
    print(f"[DEBUG] selected_columns: {selected_columns}")
    print(f"[DEBUG] top_row_indices: {top_row_indices}")
        
    selected_rows = raw_table[selected_columns].iloc[top_row_indices].reset_index(drop=True)
    TopK_filtered_df = selected_rows[selected_columns]

    return selected_columns, TopK_filtered_df, answer, question

def ablation_dataset(index):
    print("="*25,f"{index} Processing", "="*25)

    column_filtered_df, row_filtered_df, answer, question = row_column_filtering(index)
    selected_columns, TopK_filtered_df, answer, question = Top_k_vs_KMenas(index)

    print("="*25,f"{index} Completed", "="*25)

    answer = answer
    question = question
    column_filtered_df = column_filtered_df
    row_filtered_df = row_filtered_df
    TopK_filtered_df = TopK_filtered_df
    TopK_selected_columns = selected_columns

    # âœ… ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    result = {
        "index": index,
        "question": question,
        "answer": answer,
        "column_filtered_df": column_filtered_df.to_dict(orient="records"),
        "row_filtered_df": row_filtered_df.to_dict(orient="records"),
        "TopK_filtered_df": TopK_filtered_df.to_dict(orient="records"),
        "TopK_selected_columns": TopK_selected_columns
    }

    # âœ… JSONLë¡œ ì €ì¥ (append ëª¨ë“œ)
    with open("outputs/ablation_dataset.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(result, ensure_ascii=False) + "\n")

json_df = pd.read_json("Open-WikiTable_data/test.json")
valid_indices = list(json_df.index.astype(str))


from tqdm import tqdm

def run_ablation_for_all_with_tqdm():
    processed_indices = set()

    # âœ… ê¸°ì¡´ ì²˜ë¦¬ëœ index ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists("outputs/ablation_dataset.jsonl"):
        with open("outputs/ablation_dataset.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    processed_indices.add(str(entry.get("index")))
                except:
                    continue

    # âœ… tqdmìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ indexë§Œ ì¶”ë¦¼
    unprocessed_indices = [idx for idx in valid_indices if str(idx) not in processed_indices]

    print(f"ğŸš€ ì „ì²´ {len(valid_indices)}ê°œ ì¤‘, ì²˜ë¦¬ë˜ì§€ ì•Šì€ {len(unprocessed_indices)}ê°œ ì¸ë±ìŠ¤ ì‹¤í–‰ ì‹œì‘")

    for index in tqdm(unprocessed_indices, desc="Generating Ablation Dataset"):
        try:
            ablation_dataset(index)
        except Exception as e:
            print(f"\nâŒ Failed to process index {index}: {e}")

run_ablation_for_all_with_tqdm()